{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "language modelling.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/MBD-EN-BL-ENE-2020-J-1/blob/master/language_modelling/language%20modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-qIO7sguzTC"
      },
      "source": [
        "# Language Modelling\n",
        "\n",
        "In this notebook we are going to start playing with languages models. In particular, we are going to start with the simplest approach based on n-grams. Then, in the following threads, we will move to more advanced approaches based on LSTM and Transformer architectures.\n",
        "\n",
        "The Natural Language Toolkit (NLTK) has data types and functions that make life easier for us when we want to count bigrams and compute their probabilities.\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIT1xuFUuzTK"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcOKzXu1uzTL"
      },
      "source": [
        "**Import the Brown corpus**\n",
        "\n",
        "For the experimentation, we are going to use the well-known Brown Corpus.\n",
        "\n",
        "The Brown University Standard Corpus of Present-Day American Englis, or just Brown Corpus (https://en.wikipedia.org/wiki/Brown_Corpus),  is a general corpus containing 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2SYcsnQuzTL",
        "outputId": "42c09363-31b3-4734-e633-eb267675fed5"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9g71QARuzTM"
      },
      "source": [
        "From the words of the Brown corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoR82bqxuzTN",
        "outputId": "5a8dcfe7-e912-4b90-b4ee-898ae9883614"
      },
      "source": [
        "brown.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNIIvekauzTN"
      },
      "source": [
        "Let's inspect what are the most likely (most frequent) words in the dataset. The probability of a word is very important for our language model. When we ask the LM to generate new text, it should rely on these word probabilities, so it can generate words that are likely in our dataset.\n",
        "\n",
        "We compute the word frequency by using the `FreqDist` function of NLTK (an nltk.FreqDist() is like a dictionary, but it is ordered by frequency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq7nSgs0uzTN"
      },
      "source": [
        "The following uses this function to compute the freqs and plot the 20 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ousstIaUuzTO",
        "outputId": "eb099b34-f729-4267-c7fb-650d2d9a2ff6"
      },
      "source": [
        "freq_brown = nltk.FreqDist(brown.words())\n",
        "\n",
        "list(freq_brown.keys())[:20]\n",
        "freq_brown.most_common(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 62713),\n",
              " (',', 58334),\n",
              " ('.', 49346),\n",
              " ('of', 36080),\n",
              " ('and', 27915),\n",
              " ('to', 25732),\n",
              " ('a', 21881),\n",
              " ('in', 19536),\n",
              " ('that', 10237),\n",
              " ('is', 10011),\n",
              " ('was', 9777),\n",
              " ('for', 8841),\n",
              " ('``', 8837),\n",
              " (\"''\", 8789),\n",
              " ('The', 7258),\n",
              " ('with', 7012),\n",
              " ('it', 6723),\n",
              " ('as', 6706),\n",
              " ('he', 6566),\n",
              " ('his', 6466)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N5aGMWyuzTO"
      },
      "source": [
        "We can see that they are mostly stopwords, punctuation signs.\n",
        "\n",
        "**Should we remove them? Why?** \n",
        "\n",
        "No, just think in what we are trying to do here. We are trying to use the dataset to create a model of the language to, given a set of words, predict the most probable next word. For this process, stopwords, as well as punctuation or other signs are need.\n",
        "\n",
        "For the same reason, we shall not stemmize/lemmatize, neither normalize the words. We need all these variations to learn a proper language model (i.e, `the` != `The`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWVvOy8uzTP"
      },
      "source": [
        "## Bigram Model\n",
        "\n",
        "We'll start small and we will create a language model based on bi-grams. This LM is rather simplistic: it will only codify relationships of length 2.\n",
        "\n",
        "To that end, we will use the `ConditionalFreqDist` function of NLTK. `nltk.ConditionalFreqDist()` counts frequencies of pairs. When given a list of bigrams, it maps each first word of a bigram to a FreqDist over the second words of the bigram.\n",
        "\n",
        "If you remember the theoretical session, we are applying the Markov assumption: the next element (word in our case) of a sequence can be predicted by just focusing on the previous one.\n",
        "\n",
        "The following code creates these bi-gram counts.\n",
        "If we print the `conditions` we can see the antecedent of the bi-grams. (`conditions()` in a `ConditionalFreqDist` are like `keys()` in a dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLqSKRGluzTQ",
        "outputId": "a90cb250-21cd-452d-92ae-8f6828febfc1"
      },
      "source": [
        "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n",
        "cfreq_brown_2gram.conditions()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Fulton',\n",
              " 'County',\n",
              " 'Grand',\n",
              " 'Jury',\n",
              " 'said',\n",
              " 'Friday',\n",
              " 'an',\n",
              " 'investigation',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FyBBzaQuzTR"
      },
      "source": [
        "Let' see the most frequent terms after the word `my`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HZlMa8GuzTR",
        "outputId": "d5c183a3-78d2-4ec2-9449-28014bc85c5c"
      },
      "source": [
        "# the cfreq_brown_2gram entry for \"my\" is a FreqDist (i.e, a dictionary of word and freqCount).\n",
        "my_terms = cfreq_brown_2gram[\"my\"]\n",
        "\n",
        "# Sort the terms by frequency and print the 25th most common\n",
        "sorted(my_terms.items(), key=lambda x: -x[1])[:25]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'own', 52),\n",
              " (u'life', 19),\n",
              " (u'hand', 19),\n",
              " (u'mind', 19),\n",
              " (u'first', 15),\n",
              " (u'hands', 14),\n",
              " (u'wife', 14),\n",
              " (u'father', 13),\n",
              " (u'eyes', 13),\n",
              " (u'way', 12),\n",
              " (u'husband', 12),\n",
              " (u'mother', 12),\n",
              " (u'head', 11),\n",
              " (u'left', 8),\n",
              " (u'body', 7),\n",
              " (u'heart', 7),\n",
              " (u'Uncle', 7),\n",
              " (u'point', 7),\n",
              " (u'brother', 6),\n",
              " (u'family', 6),\n",
              " (u'name', 6),\n",
              " (u'best', 6),\n",
              " (u'right', 6),\n",
              " (u'business', 6),\n",
              " (u'friends', 6)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWvLg4L1uzTR"
      },
      "source": [
        "We can do the same with the `most_common` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eVkxT7DuzTS",
        "outputId": "61eed55f-27dc-4ea7-bd26-35c64f9d4686"
      },
      "source": [
        "cfreq_brown_2gram[\"my\"].most_common(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'own', 52),\n",
              " (u'life', 19),\n",
              " (u'hand', 19),\n",
              " (u'mind', 19),\n",
              " (u'first', 15),\n",
              " (u'hands', 14),\n",
              " (u'wife', 14),\n",
              " (u'father', 13),\n",
              " (u'eyes', 13),\n",
              " (u'way', 12),\n",
              " (u'husband', 12),\n",
              " (u'mother', 12),\n",
              " (u'head', 11),\n",
              " (u'left', 8),\n",
              " (u'body', 7),\n",
              " (u'heart', 7),\n",
              " (u'Uncle', 7),\n",
              " (u'point', 7),\n",
              " (u'brother', 6),\n",
              " (u'family', 6),\n",
              " (u'name', 6),\n",
              " (u'best', 6),\n",
              " (u'right', 6),\n",
              " (u'business', 6),\n",
              " (u'friends', 6)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gtx3xuTuzTS"
      },
      "source": [
        "With the `nltk.ConditionalProbDist()`, map pairs are mapped to probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Vk1D32uzTS"
      },
      "source": [
        "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist) # Uses a Maximum Likelihood Estimation (MLE) estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f8piubquzTS"
      },
      "source": [
        "This again has `conditions()` wihch are like dictionary keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeqSahgxuzTT",
        "outputId": "1b4077c7-a13d-478c-a421-383f1d1f90ad"
      },
      "source": [
        "cprob_brown_2gram.conditions()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Fulton',\n",
              " 'County',\n",
              " 'Grand',\n",
              " 'Jury',\n",
              " 'said',\n",
              " 'Friday',\n",
              " 'an',\n",
              " 'investigation',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEIp1wo_uzTT"
      },
      "source": [
        "We can also find the words that can come after `my` by using the function `samples()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOBM_KLZuzTT",
        "outputId": "a6c142c8-3a7c-4b98-ae42-a05838759a1f"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].samples()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['political', 'client', 'fellow', 'man', 'candidacy', 'best', 'place-kicking', 'last', 'reflexes', 'jobs', 'family', 'thanks', 'firm', 'payroll', 'judgment', 'sales', 'first', 'mother', 'boys', 'share', 'daily', 'wife', 'legs', 'big', 'hands', 'biologist', 'locker', 'hand', 'right', 'neck', 'heart', 'grudge', 'neighbor', 'brother', 'house', 'good', 'life', 'native', 'charge-a-plate', \"son's\", 'psychiatrist', 'son', 'children', 'arms', 'daughter', 'opinion', 'husband', 'friends', 'country', 'wonderful', 'school', 'home', 'desire', 'point', 'little', 'part', 'two', 'itinerary', 'classroom', 'initial', 'induction', 'own', 'students', 'classes', 'personal', 'only', 'estimation', 'taste', 'objectivity', 'bed', 'eyes', 'principal', 'primary', 'Roman', 'experience', 'stay', 'lot', 'leave', 'learned', 'Bible', 'nearest', 'Father', 'Saviour', 'patient', 'peace', 'work', 'patients', 'professional', 'talents', 'soul', 'light', 'salvation', 'foes', 'flesh', 'fingers', 'body', 'finger', 'word', 'lost', 'voice', 'name', '15th', 'pages', 'editorial', 'executive', 'early', 'aid', 'discouraged', 'roving', 'continued', 'three', 'father', 'turn', 'mind', 'desk', 'question', 'seedbed', ',', \"neighbors'\", 'morale', 'spirits', 'plants', 'SD', '37th', 'comments', 'double', 'scoped', 'tour', 'left', 'steps', 'privilege', 'subject', 'final', 'sketches', 'sketch', 'attack', 'studio', 'brushes', 'supposedly', 'machine', 'particular', 'girl', 'love', 'stomack', 'ink', 'Negroes', 'borough', 'second', 'thoughts', 'flashlight', 'room', 'feet', \"parents'\", 'collection', 'benefit', 'plans', 'neighbors', 'conscience', 'favorite', 'view', 'sacred', \"''\", 'being', 'information', 'usual', 'way', 'beautiful', 'need', 'liberal', \"wife's\", 'questions', 'conversations', 'binoculars', 'travel', 'dear', 'job', 'opening', 'statement', 'judge', 'cause', 'knowledge', 'actions', 'hotel', 'invention', 'plays', 'writing', 'world', 'knee', 'grandmother', 'head', 'reasons', 'feelings', 'companion', 'cold', 'burning', \"life's\", 'theme', 'employees', 'company', 'fellow-employees', 'business', 'age', 'secret', 'comrades', 'briefcase', 'money', 'recoil', 'psyche', 'bridegroom', 'fear', 'memory', 'wrongs', 'shames', 'discovery', 'intention', 'hope', 'innocence', 'general', 'trust', 'help', 'pupil', 'teaching', 'conviction', 'studied', 'honor', 'enthusiasm', 'city', 'participation', 'report', 'stories', 'state', 'derby', 'great', 'glasses', 'ability', 'goals', 'train', 'principles', 'recently', 'backpack', 'typewriter', 'regard', 'latest', 'end', '``', 'tent', 'case', 'entry', 'pallid', 'hesitation', 'visit', 'departure', 'position', 'projected', 'genius', 'destiny', 'silence', 'sad', \"song's\", 'churchgoing', 'sins', 'ultimate', 'shoulders', 'moral', 'religious', 'duty', \"father's\", \"mother's\", 'wishful', 'childishness', \"husband's\", 'L.', 'mare', 'Lorde', 'account', 'study', 'old', 'other', 'feeling', 'sons', 'books', 'Wall', 'bookshelves', 'endurance', 'votive', 'opinions', 'readers', 'youth', 'colleagues', 'attitudes', 'awareness', 'remarks', 'tendency', 'education', 'circle', 'generation', 'sentiments', 'sculptor', 'background', 'hopes', 'views', 'short', 'example', 'district', 'State', 'nine', 'esteem', 'constituents', 'newsletter', 'favorites', 'adopted', 'hunch', 'personality', 'fairly', 'failure', 'condescension', 'chair', 'Yokuts', 'full', 'remark', 'commitment', 'return', 'reactions', 'missing', 'ideal', 'honour', 'squad', 'senses', 'side', 'slovenliness', 'face', 'gun', 'flint', 'initiation', 'reality', 'recollection', 'amazement', 'shoulder', 'pants', 'arm', 'Lord', 'foot', 'child', 'people', 'pleasure', 'captain', 'crew', 'men', 'undershirt', 'clothes', 'greatest', 'sight', 'respectable', 'thirty', 'gray', 'weatherbeaten', 'vitals', 'place', 'encouragement', 'pass', 'landscape', 'list', 'kit', 'checkbook', 'regards', 'noise', 'throat', 'pot', 'hen', 'eighty-three', 'pocket', 'lungs', 'clotheshorse', 'present', 'dresser', 'scholarly', 'camp', 'street', 'mountain', 'existence', 'letters', 'imagination', 'bunkmate', 'fantasies', 'newest', 'dreams', 'adolescent', 'flashy', 'timidity', 'emotion', 'impending', 'closed', 'years', 'ear', 'wallet', 'medical', 'drinking', 'profession', 'military', 'seat', 'portrayal', 'bags', 'class', 'safe', 'change', 'ticket', 'nostrils', 'stomach', 'motel', 'evening', 'hair', 'car', 'cab', 'pockets', 'friend', 'twenty-first', 'regular', 'career', 'leg', 'office', 'references', 'trial', 'day', '275', 'road', 'watch', 'battered', 'pencil', 'Tim', 'pipe', 'forms', 'advice', 'small', 'presence', 'intelligence', 'line', 'unpadded', 'sister', '!', 'table', 'back', 'gapt', 'lessons', 'block', 'getting', 'dark', 'perennial', 'ward', 'guardian', 'property', 'mission', 'generator', 'brothers', 'elders', 'color', 'partner', 'boy', 'window', 'front', 'spare', 'rose', 'garden', 'post', 'shot', 'sights', 'knife', 'disappointment', 'frequent', 'attention', 'papers', 'past', 'dislike', 'visits', 'application', 'interview', 'knuckles', 'pardon', 'scholastic', 'motives', 'physical', 'spiritual', 'former', 'orders', 'time', 'stock', \"duds'd\", 'black', 'pilots', 'driver', 'latent', 'Spanish', 'host', \"host's\", 'supplicating', 'romantic', 'duffel', 'buying', 'shin', 'knees', 'camera', 'ears', 'kneeling', 'squatting', '(', 'astonished', 'thumb', 'elbows', 'focus', 'muffler', 'bell', 'robe', 'Colt', 'coat', 'shower', 'virtues', 'Uncle', 'normal', 'Aunt', 'new', 'looking', \"Uncle's\", 'lit', 'investment', 'jalopy', 'thighs', 'Anthropology', 'uncles', 'few', 'rosy', 'food', 'mough', 'guts', 'equipment', 'products', 'talks', 'brethren', 'fault', 'teeth', 'tongue', \"ever-lovin'\", 'meal', 'extra', 'contests', 'pin', 'yes', \"dryin'\", 'system', 'talking', 'game', 'bare', 'interest', 'skirt', 'finished', 'idea', 'paintings', 'pay', 'headlights', 'shopping', 'dentist', 'cottage', 'pity', 'very', 'person', 'fully', 'darling', 'town', 'shame', 'wish', 'promise', 'rock', 'stack', 'bureau', 'mood', 'lips', 'disquiet', 'private', 'protests', 'woman', 'dazzled', 'thinking', 'others', 'armor', 'frozen', 'topcoat', 'worries', 'purse', 'immense', 'empty', 'folks', 'faith', 'Johnnie', \"folks'\", 'truck', 'doing', 'strict', 'happiness', 'elbow', 'guest', 'release', 'watching', \"brother's\", 'pale', 'nephew', 'nerves', 'shorts', 'contacts', 'lawyer', 'God', 'consciousness', 'surprise', 'immediate', 'everyday', 'threats', 'souffle', 'diary', 'larder', 'lingerie', 'poor', 'art', 'hardest', 'unflagging', 'mouth', 'efforts', 'troubles', 'hurt', 'decision', \"daughter's\", 'chances', 'drink', 'uncle', 'dress', 'whole', 'lack', 'residence', 'dog'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8EPw4y0uzTT"
      },
      "source": [
        "In addition, you can see the prob of a particular pair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF3rUahluzTU",
        "outputId": "c9e8b11b-6976-421d-c038-ae1f0dea345a"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].prob(\"own\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04478897502153316"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPjydGm4uzTU",
        "outputId": "86e3fbee-9340-4c21-e7b4-5636cbd83968"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].prob(\"leg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0034453057708871662"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpzMKz1UuzTU"
      },
      "source": [
        "## Compute the probability of a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXtPHiQ7uzTV"
      },
      "source": [
        "Create a function to compute the probability of a word from its frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Josl8K1uzTV",
        "outputId": "f94da0f8-b9d3-4d71-913c-4df307ca02a0"
      },
      "source": [
        "def unigram_prob(word):\n",
        "    len_brown = len(brown.words())\n",
        "    return float(freq_brown[word]) / float(len_brown)\n",
        "\n",
        "unigram_prob(\"night\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003427512418273636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc9Fa4-1uzTV"
      },
      "source": [
        "We now can ask for the probability of a word sequence.\n",
        "\n",
        "For instance: `P(how do you do) = P(how) * P(do|how) * P(you|do) * P(do | you)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12JoyG54uzTV",
        "outputId": "1104e833-6fe2-4a0d-b3d8-4c5e4959408b"
      },
      "source": [
        "unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * cprob_brown_2gram[\"you\"].prob(\"do\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5639033871961e-09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkCsamOiuzTW"
      },
      "source": [
        "Compare it with the prob of another not so common sentence: `how do you dance`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI49ifcUuzTW",
        "outputId": "29f7f03b-5756-43ef-f069-0ef7402d3ff0"
      },
      "source": [
        "unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * cprob_brown_2gram[\"you\"].prob(\"dance\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0089699272232904e-10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSb3PvTruzTX"
      },
      "source": [
        "As expected, one order of magnitude less probable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWT8NGKkuzTX"
      },
      "source": [
        "## Generate Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vJk7pcpuzTX"
      },
      "source": [
        "With our bi-gram language model already generated, we can now use it to generate text and see what has our model learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjCHMKxGuzTY",
        "outputId": "752e5c82-7778-494e-a370-733df9a2bab1"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].generate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reasons'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZngBJyuzTY"
      },
      "source": [
        "Let's see if the model create valid text or just jiberish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVS1qI2cuzTY",
        "outputId": "7f58aae6-6754-4454-e74b-cd16dff29573"
      },
      "source": [
        "word = \"my\"\n",
        "text = \"\"\n",
        "for index in range(20):\n",
        "    text += word + \" \"\n",
        "    word = cprob_brown_2gram[ word].generate()\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my body caressed her off from which was anything could invoke against his own fierce heat balance alter the most \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wdev8EPuzTZ"
      },
      "source": [
        "It is not a valid sentence, but it has some kind of sense. \n",
        "\n",
        "Remember that we are just learning from bigrams!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaZbER2euzTZ"
      },
      "source": [
        "**We can try another datasets to train a language models using different dataset.**\n",
        "\n",
        "In particular we are going to import the book dataset of NLTK, which includes the text of different books.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkaYqUZ5uzTZ"
      },
      "source": [
        "The following function takes a text (i.e., the text o a given book) to learn a language model, and a initial word to start the generation and the number of words that have to be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3hpAVo9uzTZ",
        "outputId": "6f28d332-a6bb-4620-f565-1ceda8bea4ab"
      },
      "source": [
        "# Here is how to do this with NLTK books:\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "from nltk.book import *\n",
        "\n",
        "\n",
        "def generate_text(text, initialword, numwords):\n",
        "    bigrams = list(nltk.ngrams(text, 2))\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(bigrams), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        text += word + \" \"\n",
        "        word = cpd[ word].generate() \n",
        "\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /home/acastellanos/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to\n",
            "[nltk_data]     /home/acastellanos/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to\n",
            "[nltk_data]     /home/acastellanos/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to\n",
            "[nltk_data]     /home/acastellanos/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to\n",
            "[nltk_data]     /home/acastellanos/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVhYqc5HuzTa"
      },
      "source": [
        "We use different books to generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVD06Gf8uzTa",
        "outputId": "0cc690cd-8bcc-4b27-e146-6a03a88f249b"
      },
      "source": [
        "# Holy Grail\n",
        "generate_text(text6, \"I\", 25)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am your sword out , it properly . Thank you must spank me havin ' s only people did do you stupid bastard . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXk0K-vzuzTa",
        "outputId": "9a07e6e1-d860-4c7a-b4fd-417689bf49e1"
      },
      "source": [
        "# sense and sensibility\n",
        "generate_text(text2, \"I\", 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have given by it all that she was too honest . \" It was large W in a thousand pounds belonging to an age \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4s6o2hHuzTa"
      },
      "source": [
        "# TriGrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCJc1-iuzTb"
      },
      "source": [
        "Let's try a more advance model using tri-grams to see if it is able to generate better language.\n",
        "\n",
        "We cannot use the `ConditionalFreqDist` as before. `nltk.ConditionalFreqDist` expects its data as a sequence of `(condition, item)` tuples. `nltk.trigrams` returns tuples of length 3. Therefore, we have to adapt the trigrams output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZ9Rm1NuzTb"
      },
      "source": [
        "def generate_text(text, initialword, numwords):\n",
        "    trigrams = list(nltk.ngrams(text, 3,  pad_right=True, pad_left=True))\n",
        "    trigram_pairs = (((w0, w1), w2) for w0, w1, w2 in trigrams) # Adapt the format to use ConditionalFreqDist\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(trigram_pairs), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        w = cpd[(word[i],word[i+1])].generate() \n",
        "        word += [w]\n",
        "    \n",
        "    print(\" \".join(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxWLWg55uzTb",
        "outputId": "d23b44c3-38b0-4b90-e925-610e36b6a919"
      },
      "source": [
        "generate_text(text2, [\"I\", \"am\"], 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am sure it must be gained by it , need not therefore be very plenty with us just now , I would not disregard . Nothing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsDDHFXPuzTc"
      },
      "source": [
        "As expected, it creates a better lm.\n",
        "\n",
        "Can we go on with more n-grams? Let's see"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Y1TnU7uzTc"
      },
      "source": [
        "# N-grams\n",
        "\n",
        "We are going to update again the `generate_text` function to create a language model based on 4-grams.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI4GDN_JuzTc"
      },
      "source": [
        "def generate_text(text, initialword, numwords):\n",
        "    ngrams = list(nltk.ngrams(text, 4,  pad_right=True, pad_left=True))\n",
        "    ngram_pairs = (((w0, w1, w2), w3) for w0, w1, w2, w3 in ngrams)\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(ngram_pairs), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        w = cpd[(word[i],word[i+1], word[i+2])].generate() \n",
        "        word += [w]\n",
        "    \n",
        "    print(\" \".join(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIcg_O0_uzTc",
        "outputId": "ad5371e2-9a67-47dc-958a-1a458fda8151"
      },
      "source": [
        "generate_text(text2, [\"I\", \"am\", \"very\"], 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am very sure you will be a very good house for three months her companion , was still in town ?\" \" No , ma ' am\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBjSpnNwuzTd"
      },
      "source": [
        "As we make the n-grams larger we got more accurate language models. However, as explained in class, if we create large n-grams we are not going to have enough data to train our models: we will never see enough data (enough sequences of n-grams) to train the model.\n",
        "\n",
        "As an exercise, I leave up to you to keep extending this LM model to 5-gram, 6-gram....\n"
      ]
    }
  ]
}