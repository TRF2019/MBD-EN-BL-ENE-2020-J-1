{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l56xpzD4YsCp"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "I have decided to give Deep Learning a try. \n",
    "\n",
    "In the other markdown, I tried \"regular\" machine learning methodologies to solve the classification task. As I discussed there, simplicity is one of the aspects in which I would like to focus my solution on. Nevertheless, Deep Learning has become the state-of-the-art for many NLP tasks so I think that it is sensible to give it a try. A large increase in performance could motivate its application even with the increase in complexity. \n",
    "\n",
    "However, the dataset we have at our disposal includes \"only\" 65K documents. We all know that to apply Deep Learning you need tons of data and weeks of training, right? \n",
    "\n",
    "Well, this in not true, or at least, this is not anymore completely true thanks to **Transfer Learning**. If you are training a huge model from scratch, you do need a lot of data and GPU time. Lucklily for us, these huge models are already pre-trained for many languages using large datasets (e.g. Wikipedia). In more detail, these models are called **Language Models (LM)** because they are, surprisingly, ... models of your language. LM are trained on a rather simple (and silly at a first sight) task: given a sequence of words in a sentence, predict the next one. A properly trained LM should then identify that given the sentence `The dog is`, `black` is more likely than `red` (there is no red dogs). By optimizing this task, what the LM are actually doing is to learn the gramatical, lexical and semantical relationships of the language, thus grasping a deep understanding of its textual content. \n",
    "\n",
    "All we have to do is to adapt these LM to our particular domain. In layman words, the LM already speak English and we need to teach them to talk *HuffingtonPostish*. 65K documents are not enough to learn English, but they are definitely enough to learn the nuances of the dataset.\n",
    "\n",
    "Cool, now we know *HuffingtonPostish*, but the business case was to create a classifier. Why am I even doing all of this? In the traditional ML methodologies that I implemented in the other markdown, we were feeding our algorithms with a rather simple representation of our textual contents (The TF-IDF vectors). Now, thanks to the LM, I can feed my classifier with a much more detailed and accurate representation of the input textual content. Therefore, it should be easier to the classifier to better categorize the textual content.\n",
    "\n",
    "\n",
    "Summing up, my strategy will be:\n",
    "\n",
    "1.   Use a pre-trained English Language Model trained over a large dataset as starting point.\n",
    "2.   Adapt this language model to our domain. To that end, I will retrain the model to learn the particular aspects of the dataset.\n",
    "3. Create a Machine Learning Classifier on top of the dataset language model\n",
    "\n",
    "\n",
    "**Side note: I have trained this model on Google Colab to be able to use a GPU.For this reason, I did not include anything about the configuration of the GPU or the installation of the required libraries. To execute it I do recommend you to update it to Google Colab**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXkLmX5I8xGZ"
   },
   "source": [
    "## Fast AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7eq6viH8xGi"
   },
   "source": [
    "To facilitate the training of this deep learning approach, I will make use of the [fast.ai library](https://www.fast.ai/). In particular, the [`text`](https://docs.fast.ai/text.html) module of the fast.ai library contains all the necessary functions. Specifically:\n",
    "\n",
    "- [`text.transform`](https://docs.fast.ai/text.transform.html#text.transform) contains all the scripts to preprocess the data, from raw text to token ids,\n",
    "- [`text.data`](https://docs.fast.ai/text.data.html#text.data) contains the definition of [`TextDataBunch`](https://docs.fast.ai/text.data.html#TextDataBunch), which is the main class we need in NLP,\n",
    "- [`text.learner`](https://docs.fast.ai/text.learner.html#text.learner) contains helper functions to quickly create a language model or an text classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "lxu7CFCv8_xx",
    "outputId": "cab79509-d332-4936-d5c0-edfd7474c946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# I needed this to load data from my GDrive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncGqMmK68xGk"
   },
   "source": [
    "\n",
    "## Training an classifier model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbbEDziH8xGl"
   },
   "source": [
    "To create my model I am going to apply the following steps:\n",
    "\n",
    "1. Fine-tuning an [AWD-LSTM](https://arxiv.org/abs/1708.02182) model to create a language model based on our data.\n",
    "1. Building a classifier based on the learned language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrXdKN9f8xGn"
   },
   "source": [
    "### Reading and viewing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4AmXyT78xGo"
   },
   "source": [
    "First let's import everything we need for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X60QVaOO8xGq"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3070463a1496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastai'"
     ]
    }
   ],
   "source": [
    "from fastai.basics import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.text.all import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kTNNSVb8xG3"
   },
   "source": [
    "I will now read the dataset from the original JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "T8pPwgW58xG6",
    "outputId": "ce865dc3-c8d3-4b57-c612-8397d704b125"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burning</td>\n",
       "      <td>Spokane, Washington 99206</td>\n",
       "      <td>Parents are taking their kids to Burning Man and one 11 year old thinks it's 'better than... http://t.co/wp6V1BHhoQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detonate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A young heavyweight rapping off of detonate I been a leader not a lemon better get it straight ??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tragedy</td>\n",
       "      <td>houston</td>\n",
       "      <td>@itss_selenaluna like a beautiful ass tragedy lol</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mudslide</td>\n",
       "      <td>Notts</td>\n",
       "      <td>#BakeOffFriends #GBBO 'The one with the mudslide and the guy with the hat'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thunder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ebay Snipe RT? http://t.co/SlQnph34Nt Lego Power Miners Set 8960 Thunder Driller Boxed. ?Please Favorite &amp;amp; Share</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tornado</td>\n",
       "      <td>God is Love.</td>\n",
       "      <td>My room looks like a tornado passed through it and my OCD is not having it.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>debris</td>\n",
       "      <td>Belbroughton, England</td>\n",
       "      <td>#aerospace #exec Plane debris is from missing MH370 - Part of the aircraft wing found on Reunion Island is from th... http://t.co/S2wm8lh7oO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>derailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://t.co/BAGEF9lFGT 25 killed 50 injured in Madhya Pradesh twin train derailment http://t.co/bVxqA3Kfrx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trouble</td>\n",
       "      <td>?</td>\n",
       "      <td>When there's trouble you know who to caaaaaall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hailstorm</td>\n",
       "      <td>far away</td>\n",
       "      <td>Calgary news weather and traffic for August 5 * ~ 90 http://t.co/qBdRYXSGlC http://t.co/VZOd7qFFlv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>evacuate</td>\n",
       "      <td>FSC '19</td>\n",
       "      <td>I just want everyone to know that Emilee was worried I was getting a milkshake when we were supposed to evacuate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>landslide</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>FreeBesieged: .MartinMJ22 YouGov Which '#Tory landslide' ... you can't POSSIBLY mean the wafer-thin majority of #GÛ_ http://t.co/2q3fuEReY5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fire</td>\n",
       "      <td>University of South Florida</td>\n",
       "      <td>It may seem like our fire has been a little burnt out....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>crash</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>@DestinyTheGame @Bungie @PlayStation Getting kicked out by that crash is one of the worst experiences I've had playing video games.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>traumatised</td>\n",
       "      <td>cork</td>\n",
       "      <td>@AnnmarieRonan @niamhosullivanx I can't watch tat show its like a horror movie to me I get flashbacks an everything #traumatised</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>deaths</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@gregorysanders @USDOT &amp;amp; the stat of high auto deaths applies to children in a vehicle. I guess they can out run lightrail better than adult</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sirens</td>\n",
       "      <td>they/them</td>\n",
       "      <td>my dad said I look thinner than usual but really im over here like http://t.co/bnwyGx6luh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>Orlando/Cocoa Beach, FL</td>\n",
       "      <td>'Up to 40% of businesses affected by a natural or man-made disaster never reopen'\\nhttp://t.co/35JyAp0ul9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>@deadlydemi even staying up all night to he barrier for tÌüp and then having to run through a dust storm and almost passing out?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lava</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shark boy and lava girl for the third time today. I guess this is what having kids feelings like. ??????</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               keyword  ... target\n",
       "0              burning  ...      0\n",
       "1             detonate  ...      0\n",
       "2              tragedy  ...      1\n",
       "3             mudslide  ...      0\n",
       "4              thunder  ...      0\n",
       "5              tornado  ...      0\n",
       "6               debris  ...      1\n",
       "7           derailment  ...      1\n",
       "8              trouble  ...      0\n",
       "9            hailstorm  ...      0\n",
       "10            evacuate  ...      1\n",
       "11           landslide  ...      1\n",
       "12                fire  ...      0\n",
       "13               crash  ...      0\n",
       "14         traumatised  ...      0\n",
       "15              deaths  ...      0\n",
       "16              sirens  ...      1\n",
       "17  natural%20disaster  ...      0\n",
       "18        dust%20storm  ...      1\n",
       "19                lava  ...      0\n",
       "\n",
       "[20 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_csv('./nlp_disaster/data/train.csv', sep=',', index_col=0)\n",
    "training_df = training_df.sample(frac=1).reset_index(drop=True)\n",
    "training_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "uOx1Jmd59Mon",
    "outputId": "3c94cfd5-f2ec-4e7d-a19b-b756b7fec1d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword  ...                                                                                              text\n",
       "id          ...                                                                                                  \n",
       "0      NaN  ...                                                                Just happened a terrible car crash\n",
       "2      NaN  ...                                  Heard about #earthquake is different cities, stay safe everyone.\n",
       "3      NaN  ...  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\n",
       "9      NaN  ...                                                          Apocalypse lighting. #Spokane #wildfires\n",
       "11     NaN  ...                                                     Typhoon Soudelor kills 28 in China and Taiwan\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./nlp_disaster/data/test.csv', sep=',', index_col=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDSdT0CS8xG_"
   },
   "source": [
    "### Getting your data ready for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUCCkCV68xHF"
   },
   "source": [
    "fast.ai provides some helpful loaders to format the input data to the format required by the Deep Learning Model.\n",
    "\n",
    "The main structure we need to create is the [`DataBunch`](https://docs.fast.ai/basic_data.html#DataBunch) that creates the training and validation sets by grabbing the textual data from the required columns.\n",
    "\n",
    "Here we'll use the method <code>from_df</code> of the [`TextLMDataBunch`](https://docs.fast.ai/text.data.html#TextLMDataBunch) (to get the data ready for the fine-tuning of the language model) and [`TextClasDataBunch`](https://docs.fast.ai/text.data.html#TextClasDataBunch) (to get the data ready for the classification step) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "5DPll640ZKye",
    "outputId": "df2778a9-741b-485e-8a5f-c8ad061c4e42"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Language model data\n",
    "df_all = pd.concat([training_df, test_df])\n",
    "splits = [list(range_of(training_df)), list(range(len(training_df), len(df_all)))]\n",
    "tfms = [attrgetter(\"text\"), Tokenizer.from_df(2), Numericalize()]\n",
    "dsets = Datasets(df_all, [tfms], splits=splits, dl_type=LMDataLoader)\n",
    "\n",
    "bs,sl = 104,25\n",
    "dls = dsets.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "nu_enqXTbF3L",
    "outputId": "f98a6440-284b-48e0-af02-0cbb19c23470"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxup u.s . xxmaj forest xxmaj service firefighter xxmaj david xxmaj xxunk xxunk died in the ' frog xxmaj fire ' in the xxmaj</td>\n",
       "      <td>xxup u.s . xxmaj forest xxmaj service firefighter xxmaj david xxmaj xxunk xxunk died in the ' frog xxmaj fire ' in the xxmaj xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bloody xxunk now xxbos xxmaj path of xxmaj obliteration \\n xxmaj back xxmaj from xxmaj the xxmaj dead \\n xxmaj story by xxunk xxunk \\n\\n▁</td>\n",
       "      <td>xxunk now xxbos xxmaj path of xxmaj obliteration \\n xxmaj back xxmaj from xxmaj the xxmaj dead \\n xxmaj story by xxunk xxunk \\n\\n▁ http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usgs xxup eq : m 1.9 - 15 km e of xxmaj anchorage xxmaj alaska : xxmaj time2015 - 08 - 06 00:11:16 xxup utc2015</td>\n",
       "      <td>xxup eq : m 1.9 - 15 km e of xxmaj anchorage xxmaj alaska : xxmaj time2015 - 08 - 06 00:11:16 xxup utc2015 -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspect killed in xxunk … http : / / t.co / xxunk xxbos xxmaj xxunk xxmaj rousey would be ' close ' to making xxmaj</td>\n",
       "      <td>killed in xxunk … http : / / t.co / xxunk xxbos xxmaj xxunk xxmaj rousey would be ' close ' to making xxmaj floyd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>latest drought conditions in your area https : / / t.co / xxunk xxbos xxmaj now xxmaj playing : xxmaj landslide by xxmaj xxunk xxmaj</td>\n",
       "      <td>drought conditions in your area https : / / t.co / xxunk xxbos xxmaj now xxmaj playing : xxmaj landslide by xxmaj xxunk xxmaj xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/ t.co / xxunk xxmaj minecraft : xxup night xxup lucky xxup block xxup mod ( bob xxup apocalypse xxup wither 2.0 &amp; &amp; xxup</td>\n",
       "      <td>t.co / xxunk xxmaj minecraft : xxup night xxup lucky xxup block xxup mod ( bob xxup apocalypse xxup wither 2.0 &amp; &amp; xxup more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxmaj wounded xxmaj families | xxup xxunk - xxmaj the xxmaj world xxmaj seen xxmaj from xxmaj rome http : / / t.co / xxunk</td>\n",
       "      <td>wounded xxmaj families | xxup xxunk - xxmaj the xxmaj world xxmaj seen xxmaj from xxmaj rome http : / / t.co / xxunk xxbos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/ / t.co / xxunk xxbos xxunk xxmaj yes xxmaj i 'm a bleeding heart xxunk . xxbos xxmaj road xxmaj hazard @ xxup xxunk</td>\n",
       "      <td>/ t.co / xxunk xxbos xxunk xxmaj yes xxmaj i 'm a bleeding heart xxunk . xxbos xxmaj road xxmaj hazard @ xxup xxunk xxup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t.co / xxunk - / xxbos xxmaj pic of 16yr old xxup pkk suicide bomber who detonated bomb in xxmaj turkey xxmaj army trench released</td>\n",
       "      <td>/ xxunk - / xxbos xxmaj pic of 16yr old xxup pkk suicide bomber who detonated bomb in xxmaj turkey xxmaj army trench released http</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Kny3tdF8xHN"
   },
   "source": [
    "\n",
    "Since this step can be a bit time-consuming, it's best to save the result with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrlBNkbQ8xHT"
   },
   "source": [
    "You can then reload those results with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jIV2NPE8xHd"
   },
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOGZ-GnU8xHf"
   },
   "source": [
    "We can now use the `data_lm` object I created earlier to fine-tune a pretrained language model. [fast.ai](http://www.fast.ai/) has an English model with an [AWD-LSTM architecture](https://arxiv.org/abs/1708.02182) available. To use it, we can create a learner object that will directly create the model, download the pretrained weights and be ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76pJ6TVBXG1H"
   },
   "source": [
    "Neural nets in general, and the AWD_LSTM model in particular, are well know for having a huge number of hyperparameters to optimize. \n",
    "\n",
    "I am deliberately using the default values that fast.ai implements. fast.ai is well know for their superconvergent models that are able to train in just a few steps. This is done through their thoughtful research about initializations, regularization, optimizers and batch normalization. All of these findings are implmented in their default values, so, for most of the tasks, we can safely use them.\n",
    "\n",
    "There is though an hyperparameter that requires careful optimization, the learning rate. Learning rate refers to the \"speed\" at which the optimizer is updating the neural net parameters in the backpropagation step. Pick a large learning rate and your NN will never converge. Pick a small rate and you will wait forever to see your NN achive a good performance or to see your NN stuck at a local minima. \n",
    "\n",
    "I will make use of the `lr_find` method in fast.ai to make sense of the values that are optimal for my data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K34SbNbAbU_8"
   },
   "outputs": [],
   "source": [
    "config = awd_lstm_lm_config.copy()\n",
    "config.update({'input_p': 0.6, 'output_p': 0.4, 'weight_p': 0.5, 'embed_p': 0.1, 'hidden_p': 0.2})\n",
    "model = get_language_model(AWD_LSTM, len(dls.vocab), config=config)\n",
    "\n",
    "opt_func = partial(Adam, wd=0.1, eps=1e-7)\n",
    "cbs = [MixedPrecision(clip=0.1), ModelReseter, RNNRegularizer(alpha=2, beta=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5pnMyxRYg7x"
   },
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), opt_func=opt_func, cbs=cbs, metrics=[accuracy, Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "5RBC55DQYluA",
    "outputId": "c904eb90-2495-49fc-beb5-26cef13a79b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0022692142054438593, lr_steep=4.2668071387197415e-07)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEKCAYAAADn+anLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZRXmiSUUkp5pUlCKaWUV5oklFJKefX/r9c7Tds+GukAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-7, end_lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8ZCjmtLZnmo"
   },
   "source": [
    "What we see in this figure is the projection of how the model will perform based on different learning rates.\n",
    "\n",
    "We can see that around 5e-01 the training starts to go south (i.e., the loss/error starts rocketing). The minimal loss is around 1e-1. Nevertheless, we have to choose a value that is approximately in the middle of the sharpest downward slope. Why?! Well, for a more intuitive explanation you can check this [blog post](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html). Basically, the reason is:\n",
    "\n",
    "> [...] the minimum value is already a bit too high, since we are at the edge between improving and getting all over the place. We want to go one order of magnitude before, a value that's still aggressive (so that we train quickly) but still on the safe side from an explosion.\n",
    "\n",
    "\n",
    "This is given as an indication by the LR Finder tool, so let's try 1e-2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "RVbMmZmg8xHg",
    "outputId": "23c21708-6a3e-429e-ab29-7b19951e1ef1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.135139</td>\n",
       "      <td>5.494511</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>243.352524</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.126870</td>\n",
       "      <td>5.493145</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>243.020187</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.128255</td>\n",
       "      <td>5.488292</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>241.843719</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.126372</td>\n",
       "      <td>5.487077</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>241.550171</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.118368</td>\n",
       "      <td>5.485628</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>241.200394</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.117770</td>\n",
       "      <td>5.482721</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>240.500183</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.109224</td>\n",
       "      <td>5.477413</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>239.227066</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.096096</td>\n",
       "      <td>5.477844</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>239.330093</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.092522</td>\n",
       "      <td>5.472507</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>238.056137</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.084905</td>\n",
       "      <td>5.473016</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>238.177521</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.083810</td>\n",
       "      <td>5.468289</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>237.054337</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>6.072297</td>\n",
       "      <td>5.462651</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>235.721436</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.063723</td>\n",
       "      <td>5.462663</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>235.724365</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.051831</td>\n",
       "      <td>5.460444</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>235.201935</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>6.033651</td>\n",
       "      <td>5.458073</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>234.644745</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.029563</td>\n",
       "      <td>5.457872</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>234.597656</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>6.016027</td>\n",
       "      <td>5.456591</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>234.297379</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.011304</td>\n",
       "      <td>5.456560</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>234.290115</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.008166</td>\n",
       "      <td>5.456583</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>234.295578</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.006659</td>\n",
       "      <td>5.456589</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>234.296814</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, 0.0022, moms=(0.8,0.7,0.8), div=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLC5O_ji8xHk"
   },
   "source": [
    "After just only one epoch we have a pretty good result (i.e., remember that we are still fine-tuning the language model, so guessing right about one third of times which is going to be the next word is a pretty good model). It seems that the default values of the library based on the super-convergence approach are just fine. So, I am not going to pay attention to them anymore. In a more developed solution, we could spend some time trying to optimize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl1vF5fscBMx"
   },
   "source": [
    "At this point it is important to explain what are we actually training. As I explained before, we have a pre-trained model of the English language. By pretrained, we mean nothing more the Neural Net has a set of weights already learned.\n",
    "This model is quite deep, it has many number of layers with millions of parameters (a.k.a. weights) to learn. It has been posible to train such a huge model, because the entire Wikipedia was used. \n",
    "\n",
    "However, we only have a small bunch of news reports to adapt/fine-tune the model to our domain. Fine-tuning is nothing more than modifying a little bit the weights of the model, so they reflect the particular relationships in our data. But, since our dataset is rather small, if we try to retrain all the layers in the model, we will completely destroy it (the model will catastrophically forget everything learned from Wikipedia). \n",
    "\n",
    "If you think about it, it makes sense, the deeper layers in the model are actually learning the basic aspects of the language, while shallow layers are in charge of more high-level relationships. We do not want to change the deeper layers. Basic language relationships are the same in our dataset than in the entire English language (e.g., verb tenses, subject-object relationships). What we actually want to modify is the last layers of the model in charge of learning the high level relationships (e.g. Obama does something). Well, that's exactly what the default training method in fast.ai is doing: It freezes the deeper layers so you do not modify them while you are fine-tuning your model. How convenient, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2tCZIx4pVR-"
   },
   "source": [
    "In the previous step, we have fine-tuned the LM to the particular content of our dataset. To do this, we just trained the last layers in our model to avoid forgetting everything. Now that our model is in a *stable* state (it has learned from our data without losing what learned from Wikipedia), we can further train our model by unfreeze all the layers.\n",
    "\n",
    "The rationale is the following: we can slightly change the weights of all the layers so even the most basic aspects of the language can be redefined according to our data (e.g. Napoleonic wars Wikipedia pages can learn the model that France<-->invade is a likely outcome; however, I do not expect to read anything about a French invasion in the HuffPost). \n",
    "\n",
    "It has been experimentally proven that this process can improve the performane of Deep Learning models (see this [Howard and Ruder's paper](https://arxiv.org/pdf/1801.06146.pdf) for more details). We have to be extremely careful with this process, otherwise we will ruin all our work. To that end, I have reduced my learning rate by an order of magnitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "M535Cjn28xHm",
    "outputId": "0d1d4375-059e-4aa3-f1cf-0995680def7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.005759</td>\n",
       "      <td>5.457955</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>234.617111</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.013834</td>\n",
       "      <td>5.459658</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>235.017075</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.013258</td>\n",
       "      <td>5.460328</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>235.174469</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.004877</td>\n",
       "      <td>5.460502</td>\n",
       "      <td>0.119225</td>\n",
       "      <td>235.215393</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.007364</td>\n",
       "      <td>5.460192</td>\n",
       "      <td>0.119270</td>\n",
       "      <td>235.142502</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.012168</td>\n",
       "      <td>5.459572</td>\n",
       "      <td>0.119248</td>\n",
       "      <td>234.996902</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.005960</td>\n",
       "      <td>5.459644</td>\n",
       "      <td>0.119248</td>\n",
       "      <td>235.013702</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.006225</td>\n",
       "      <td>5.459207</td>\n",
       "      <td>0.119282</td>\n",
       "      <td>234.911072</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.004986</td>\n",
       "      <td>5.459043</td>\n",
       "      <td>0.119316</td>\n",
       "      <td>234.872543</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.002782</td>\n",
       "      <td>5.458996</td>\n",
       "      <td>0.119316</td>\n",
       "      <td>234.861572</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 0.00022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMCYWM-ZtBOH"
   },
   "source": [
    "Cool! Only one epoch and my model has been improved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4DYmf_G8xHs"
   },
   "source": [
    "To understand what our language model is learning, you can run the [`Learner.predict`](https://docs.fast.ai/basic_train.html#Learner.predict) method and specify the number of words you want it to guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "60xLjz1D8xHu",
    "outputId": "546c8ce7-fe64-488d-eeb9-9e16c45a7e7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-38681850c891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Haha South Tampa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, item, rm_type_tfms, with_input)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm_type_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm_type_tfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrm_type_tfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_decoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_inp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmgr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mctx_mgrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_epoch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minner\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_before_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_epoch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minner\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_after_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36m_do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelValidException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_validate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_validate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mordered_cbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msort_by_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m              \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m              else f.__getitem__)\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, items, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_xtra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_newchk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, items, use_list, match, *rest)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_list\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_listify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_coll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m_listify\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_Arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mfargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Arg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpargs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msort_by_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_bn_bias_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mbn_bias_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/learner.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msort_by_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_bn_bias_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mbn_bias_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/callback/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, event_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m         _run = (event_name not in _inner_loop or (self.run_train and getattr(self, 'training', True)) or\n\u001b[1;32m     23\u001b[0m                (self.run_valid and not getattr(self, 'training', False)))\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'after_fit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;31m#Reset self.run to True at each end of fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/callback/core.py\u001b[0m in \u001b[0;36mafter_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mafter_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;34m\"Concatenate all recorded tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_input\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdetuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_preds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdetuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_targs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai2/torch_core.py\u001b[0m in \u001b[0;36mto_concat\u001b[0;34m(xs, dim)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m\"Concat the element in `xs` (recursively if they are tuples/lists of tensors)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m#We may receives xs that are not concatenatable (inputs of a text classifier for instance),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  print(learn.predict(\"Haha South Tampa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khzniEGX8xHz"
   },
   "source": [
    "It doesn't make much sense (we have a tiny vocabulary here and didn't train much on it) but note that it respects basic grammar (which comes from the pretrained model) while it has adapted the conversation to the content of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zUkM4xZuI4B"
   },
   "source": [
    "\n",
    "I could futher train or experiment with more hyperparameters, but since the language model is not the final outcome and the LM seems good enough, I will move on to the classification step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTQymGWCuOLX"
   },
   "source": [
    "We can save the model for later uses. We need to save the encoder (the part that given a textual content creates the representation) which is the only part of the model needed for classification in the next section (i.e., the other part is the decoder in charge of translating the textual representation back to the text again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIVKMQ688xH0"
   },
   "outputs": [],
   "source": [
    "learn.save_encoder('/content/drive/My Drive/nlp_disaster/models/ft_enc_extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a38ilFdZ8xH4"
   },
   "source": [
    "### Building a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaT8zAK08xH5"
   },
   "source": [
    "It's now time to actually create the classifier taking our fine-tuned encoder. For this step we need the `data_clas` object we created earlier.\n",
    "\n",
    "For this step, I will use again the default values in fast.ai to create a classification model. In this case, I will use the `text_classifier_learner`. We need to also tell the classifier that we want to use the fine-tuned encoder that we have just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "prrNRWVC8xH7",
    "outputId": "cb77611b-8511-454a-c622-6acfec568ce3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (6613 items)\n",
       "x: TextList\n",
       "xxbos xxunk xxmaj so going to make any bomb threats ? xxunk,xxbos xxmaj path of xxmaj obliteration \n",
       "  xxmaj back xxmaj from xxmaj the xxmaj dead \n",
       "  xxmaj story by xxunk xxunk \n",
       " \n",
       "  http : / / t.co / xxunk,xxbos xxmaj element of xxmaj freedom : xxmaj the xxmaj biggest xxmaj party of the xxmaj summer @ xxmaj mirage xxmaj saturday ! xxmaj tickets at http : / / t.co / 7hanpcr5rk,xxbos xxmaj is it possible to sneak into a hospital so i can stab myself with a hazardous needle and xxunk some crazy disease into my xxunk until i die,xxbos # pbban ( xxmaj temporary:300 ) xxunk @'armageddon | xxup do xxup not xxup kill | xxup flags xxup only | xxmaj fast xxup xp ' for xxmaj reason\n",
       "y: CategoryList\n",
       "0,1,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj flooding kills 166 displace over one million in xxmaj pakistan http : / / t.co / xxunk \n",
       " \n",
       "  xxmaj at least 166 people have been killed and nearly 11 xxunk _,xxbos xxmaj hat # russian soviet army xxunk military # xxunk # xxunk xxup link : \n",
       "  http : / / t.co / xxunk http : / / t.co / xxunk,xxbos xxunk i still got video of u demolished,xxbos xxmaj people who say it can not be done should not xxunk those who are doing it . xxup ûò xxmaj george xxmaj xxunk xxmaj xxunk,xxbos xxmaj even then our words xxunk and souls xxunk xxmaj xxunk than xxunk xxunk xxmaj just as we collide http : / / t.co / xxunk\n",
       "y: CategoryList\n",
       "1,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (3263 items)\n",
       "x: TextList\n",
       "xxbos xxmaj just happened a terrible car crash,xxbos xxmaj heard about # earthquake is different cities xxunk stay safe everyone .,xxbos there is a forest fire at spot pond xxunk xxunk are fleeing across the street xxunk i can not save them all,xxbos xxmaj apocalypse lighting . # xxmaj xxunk # wildfires,xxbos xxmaj typhoon xxmaj soudelor kills 28 in xxmaj china and xxmaj taiwan\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ., model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(6240, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(6240, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f8d6e34dd08>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (6613 items)\n",
       "x: TextList\n",
       "xxbos xxunk xxmaj so going to make any bomb threats ? xxunk,xxbos xxmaj path of xxmaj obliteration \n",
       "  xxmaj back xxmaj from xxmaj the xxmaj dead \n",
       "  xxmaj story by xxunk xxunk \n",
       " \n",
       "  http : / / t.co / xxunk,xxbos xxmaj element of xxmaj freedom : xxmaj the xxmaj biggest xxmaj party of the xxmaj summer @ xxmaj mirage xxmaj saturday ! xxmaj tickets at http : / / t.co / 7hanpcr5rk,xxbos xxmaj is it possible to sneak into a hospital so i can stab myself with a hazardous needle and xxunk some crazy disease into my xxunk until i die,xxbos # pbban ( xxmaj temporary:300 ) xxunk @'armageddon | xxup do xxup not xxup kill | xxup flags xxup only | xxmaj fast xxup xp ' for xxmaj reason\n",
       "y: CategoryList\n",
       "0,1,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj flooding kills 166 displace over one million in xxmaj pakistan http : / / t.co / xxunk \n",
       " \n",
       "  xxmaj at least 166 people have been killed and nearly 11 xxunk _,xxbos xxmaj hat # russian soviet army xxunk military # xxunk # xxunk xxup link : \n",
       "  http : / / t.co / xxunk http : / / t.co / xxunk,xxbos xxunk i still got video of u demolished,xxbos xxmaj people who say it can not be done should not xxunk those who are doing it . xxup ûò xxmaj george xxmaj xxunk xxmaj xxunk,xxbos xxmaj even then our words xxunk and souls xxunk xxmaj xxunk than xxunk xxunk xxmaj just as we collide http : / / t.co / xxunk\n",
       "y: CategoryList\n",
       "1,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (3263 items)\n",
       "x: TextList\n",
       "xxbos xxmaj just happened a terrible car crash,xxbos xxmaj heard about # earthquake is different cities xxunk stay safe everyone .,xxbos there is a forest fire at spot pond xxunk xxunk are fleeing across the street xxunk i can not save them all,xxbos xxmaj apocalypse lighting . # xxmaj xxunk # wildfires,xxbos xxmaj typhoon xxmaj soudelor kills 28 in xxmaj china and xxmaj taiwan\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ., model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(6240, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(6240, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f8d6e34dd08>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(6240, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(6240, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0, MixedPrecision\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (6613 items)\n",
       "x: TextList\n",
       "xxbos xxunk xxmaj so going to make any bomb threats ? xxunk,xxbos xxmaj path of xxmaj obliteration \n",
       "  xxmaj back xxmaj from xxmaj the xxmaj dead \n",
       "  xxmaj story by xxunk xxunk \n",
       " \n",
       "  http : / / t.co / xxunk,xxbos xxmaj element of xxmaj freedom : xxmaj the xxmaj biggest xxmaj party of the xxmaj summer @ xxmaj mirage xxmaj saturday ! xxmaj tickets at http : / / t.co / 7hanpcr5rk,xxbos xxmaj is it possible to sneak into a hospital so i can stab myself with a hazardous needle and xxunk some crazy disease into my xxunk until i die,xxbos # pbban ( xxmaj temporary:300 ) xxunk @'armageddon | xxup do xxup not xxup kill | xxup flags xxup only | xxmaj fast xxup xp ' for xxmaj reason\n",
       "y: CategoryList\n",
       "0,1,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (1000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj flooding kills 166 displace over one million in xxmaj pakistan http : / / t.co / xxunk \n",
       " \n",
       "  xxmaj at least 166 people have been killed and nearly 11 xxunk _,xxbos xxmaj hat # russian soviet army xxunk military # xxunk # xxunk xxup link : \n",
       "  http : / / t.co / xxunk http : / / t.co / xxunk,xxbos xxunk i still got video of u demolished,xxbos xxmaj people who say it can not be done should not xxunk those who are doing it . xxup ûò xxmaj george xxmaj xxunk xxmaj xxunk,xxbos xxmaj even then our words xxunk and souls xxunk xxmaj xxunk than xxunk xxunk xxmaj just as we collide http : / / t.co / xxunk\n",
       "y: CategoryList\n",
       "1,0,0,0,0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (3263 items)\n",
       "x: TextList\n",
       "xxbos xxmaj just happened a terrible car crash,xxbos xxmaj heard about # earthquake is different cities xxunk stay safe everyone .,xxbos there is a forest fire at spot pond xxunk xxunk are fleeing across the street xxunk i can not save them all,xxbos xxmaj apocalypse lighting . # xxmaj xxunk # wildfires,xxbos xxmaj typhoon xxmaj soudelor kills 28 in xxmaj china and xxmaj taiwan\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ., model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(6240, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(6240, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f8d6e34dd08>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(6240, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(6240, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)\n",
       "loss_scale: 65536\n",
       "max_noskip: 1000\n",
       "dynamic: True\n",
       "clip: None\n",
       "flat_master: False\n",
       "max_scale: 16777216\n",
       "loss_fp32: True], layer_groups=[Sequential(\n",
       "  (0): Embedding(6240, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(6240, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()\n",
    "learn.load_encoder('/content/drive/My Drive/nlp_disaster/models/ft_enc_extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "6-F9CfhP430M",
    "outputId": "b41f79d3-6323-4d75-e733-d7de2ba42c15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fbeta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='89' class='' max='206', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      43.20% [89/206 00:01<00:01 0.7803]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej0rANCquU09"
   },
   "source": [
    "Thanks to fast.ai super-convergence strategies, I think that just one epoch should be enough. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "Ou8W_Imd8xIJ",
    "outputId": "d03dbc2a-199a-4839-a671-b78e6f588597"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.570721</td>\n",
       "      <td>0.544005</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io2s9yiImVAH"
   },
   "outputs": [],
   "source": [
    "learn.save('./nlp_disaster/models/1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz7J-croud_D"
   },
   "source": [
    "Great! As I expected. 87% of accuracy.\n",
    "\n",
    "This is in the line of the accuracy reported by the simpler models that I already implemented in the other markdown. However, we need to compare them in terms of the F1 macro AVG as we discussed. In additon we can futher train this model. As in the language modeling training, we have just trained the last layers of the model, we can gradually unfreeze the rest of the model and train it together.\n",
    "\n",
    "In this case I will not unfreeze all the model at a time, but in different steps. This gradual unfreezing has been experimentally proven to improve the training process (again, check Howard and Ruder's paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "twCBr7mL8xIO",
    "outputId": "2a535cf1-d093-4182-99c2-979fa5be0900"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.545875</td>\n",
       "      <td>0.525755</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL-ALzPzmbwO"
   },
   "outputs": [],
   "source": [
    "learn.save('./nlp_disaster/models/2nd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "nzpVi542mle_",
    "outputId": "9d60d1e3-82b0-448b-eec6-a432fc6a8c46"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.505258</td>\n",
       "      <td>0.506216</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(1e-2/2/(2.6**4),1e-2/2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-Z_xXW_mpWG"
   },
   "outputs": [],
   "source": [
    "learn.save('./nlp_disaster/models/models/3rd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "sJVT_NlT8xIS",
    "outputId": "f6d2c70f-efc0-42a9-e6c8-6650d9499635"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='19' class='' max='20', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      95.00% [19/20 02:48<00:08]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.485252</td>\n",
       "      <td>0.522730</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485891</td>\n",
       "      <td>0.506821</td>\n",
       "      <td>0.761000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.474911</td>\n",
       "      <td>0.505298</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.477685</td>\n",
       "      <td>0.506820</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.453993</td>\n",
       "      <td>0.524493</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.439716</td>\n",
       "      <td>0.514499</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.453190</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.441064</td>\n",
       "      <td>0.518208</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.420555</td>\n",
       "      <td>0.526317</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.405216</td>\n",
       "      <td>0.518275</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.408657</td>\n",
       "      <td>0.523087</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.386781</td>\n",
       "      <td>0.525924</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.382440</td>\n",
       "      <td>0.547991</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.396783</td>\n",
       "      <td>0.546178</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.362195</td>\n",
       "      <td>0.556745</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.368544</td>\n",
       "      <td>0.550799</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.357226</td>\n",
       "      <td>0.559059</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.345088</td>\n",
       "      <td>0.557131</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.343327</td>\n",
       "      <td>0.555915</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='60' class='' max='206', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      29.13% [60/206 00:02<00:06 0.3401]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(20, slice(1e-2/10/(2.6**4),1e-2/10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0moaKqhImyLI"
   },
   "outputs": [],
   "source": [
    "learn.save('./nlp_disaster/models/classifier_5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02DIp2_ov37s"
   },
   "source": [
    "As can be seen in the results, our model is now offering accuracies up to 95%! Better than those we had with LR, RF or SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIJKAxsP8xIX"
   },
   "source": [
    "Again, we can predict on a raw text (the category in this case) by using the [`Learner.predict`](https://docs.fast.ai/basic_train.html#Learner.predict) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "Zpp13v1N8xIY",
    "outputId": "98b48384-57c6-4e70-92da-9bce125cb154"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Category D,\n",
       " tensor(3),\n",
       " tensor([2.6761e-01, 1.5789e-03, 1.7408e-03, 6.0898e-01, 1.7072e-03, 2.9603e-02,\n",
       "         4.3496e-05, 8.4169e-05, 8.8507e-02, 1.4714e-04]))"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"This Artist Gives Renaissance-Style Sculptures...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZTY2buWwMBx"
   },
   "source": [
    "In order to have a full report of this model performance, I will ask the model to offer me all the predictions and inspect the confusion matrix and the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "LglwOWs5iUuh",
    "outputId": "0cc96002-9f2b-457f-ec78-9950c6a2233d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1523</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>1727</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>90</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1100</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "      <td>1826</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>44</td>\n",
       "      <td>135</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1987</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>5947</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12624</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2392</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>2521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0     1     2     3     4     5      6     7    8     9\n",
       "row_0                                                            \n",
       "0      1523     9     2    15     1     8      1    28   14     2\n",
       "1        29  1727    28    12     2    16     19    90   25     7\n",
       "2         2    29  1100    19     2     2     24    35   53     4\n",
       "3       194    24   150  1826    22    26     44   135   28    17\n",
       "4         0     4     0    12  1987    36      2     0    3    30\n",
       "5        42    32     3    23    29  5947      7    11   11    47\n",
       "6         2    10    14    12     1     4  12624     6    2     3\n",
       "7        17    35    18    19     1     2      2  2392    6     1\n",
       "8        51    60    81    15     3     3      5    40  521     0\n",
       "9        18    15     9    18    29    54     11    19    7  2521"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions\n",
    "preds, targets = learn.get_preds()\n",
    "\n",
    "predictions = np.argmax(preds, axis = 1)\n",
    "pd.crosstab(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "0PugeNbGkWF6",
    "outputId": "02ccfaca-bf6e-4866-f396-6fb7946bc177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.88      1878\n",
      "           1       0.88      0.89      0.89      1945\n",
      "           2       0.87      0.78      0.82      1405\n",
      "           3       0.74      0.93      0.82      1971\n",
      "           4       0.96      0.96      0.96      2077\n",
      "           5       0.97      0.98      0.97      6098\n",
      "           6       1.00      0.99      0.99     12739\n",
      "           7       0.96      0.87      0.91      2756\n",
      "           8       0.67      0.78      0.72       670\n",
      "           9       0.93      0.96      0.95      2632\n",
      "\n",
      "    accuracy                           0.94     34171\n",
      "   macro avg       0.89      0.89      0.89     34171\n",
      "weighted avg       0.95      0.94      0.94     34171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EE_Z3qRwfZ3"
   },
   "source": [
    "As can be seen in both the confusion matrix and in the classification report, the model is better not only in terms of accuracy, but also in its performance for all the classes when compared to the non Deep Learning models. Something that I care about since the start.\n",
    "\n",
    "As expected, there are some classes for which the model performs better (they are easier to predict or we have more data for them). However, we get a reasonable performance for most of the classes (I am a little bit worried by class 8, but since we do not have many data I guess I can accept this result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPE9ufJowcT2"
   },
   "source": [
    "Finally, saving the model object to reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Zdg1Ay2ho97"
   },
   "outputs": [],
   "source": [
    "learn.save('.nlp_disaster/models/final_class_model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "v2_deep_learning_solution.ipynb",
   "provenance": []
  },
  "jekyll": {
   "keywords": "fastai",
   "summary": "Application to NLP, including ULMFiT fine-tuning",
   "title": "text"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
