{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.rendered_html { font-size: 18px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Larger window and fontsize\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.rendered_html { font-size: 18px; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Sentiment Analysis refers to the use of text analysis and natural language processing to identify and extract subjective information in textual contents.\n",
    "\n",
    "In this practice we will focus on the analysis of the sentiment of a collection of tweets, applying some of the ideas that we have explored in class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This corpus of tweets, developed by Stanfordâ€™s Natural Language processing research group\n",
    "The training set is collected by querying Twitter API for happy emoticons like \":)\" and sad emoticons like \":(\" and labelling them positive or negative. The emoticons were then stripped and Re-Tweets and duplicates removed.\n",
    "\n",
    "The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "\n",
    "    0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) **Note**: For the dataset there is only negative and positive tweets\n",
    "    1 - the id of the tweet (2087)\n",
    "    2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    4 - the user that tweeted (robotickilldozr)\n",
    "    5 - the text of the tweet (Lyx is cool)\n",
    "\n",
    "It also contains around 500 tweets manually collected and labelled for testing purposes.\n",
    "\n",
    "We randomly sample and use 5000 tweets from this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a folder to store the dataset and download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir stanford_dataset\n",
    "! wget --directory-prefix=stanford_dataset/ https://github.com/crwong/cs224u-project/raw/master/data/sentiment/training.1600000.processed.noemoticon.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "def loadDataset(in_file):\n",
    "    my_path = os.getcwd()\n",
    "    path = os.path.join(my_path, in_file)\n",
    "    column_names = ['sentiment','ID', 'Date', 'Query', 'user_id', 'tweet']\n",
    "    tweets = pd.read_csv(path, delimiter=',', quotechar='\"', header= None, names= column_names, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    print('Readed ', len(tweets), \"tweets\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and check the number of positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed  1600000 tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    800000\n",
       "4    800000\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data = loadDataset(\"datasets/stanford_dataset/training.1600000.processed.noemoticon.csv\")\n",
    "raw_training_data.groupby('sentiment')['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains more than a million tweets, for our practice we will only use a sample of 5000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418520</th>\n",
       "      <td>0</td>\n",
       "      <td>2061788339</td>\n",
       "      <td>Sat Jun 06 21:37:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Halfbeanchacha</td>\n",
       "      <td>@crazyycamille my dads being a jerk and won't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447784</th>\n",
       "      <td>0</td>\n",
       "      <td>2068796410</td>\n",
       "      <td>Sun Jun 07 14:42:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>VoniaPerna</td>\n",
       "      <td>@ mmm that does sound good, but i'm at work  H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326165</th>\n",
       "      <td>0</td>\n",
       "      <td>2008025357</td>\n",
       "      <td>Tue Jun 02 13:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>audy86</td>\n",
       "      <td>Oooo god it's humid ! I hate humidity!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481308</th>\n",
       "      <td>0</td>\n",
       "      <td>2179593331</td>\n",
       "      <td>Mon Jun 15 09:19:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexyexecutive</td>\n",
       "      <td>Low toner error at 5:19pm  Bloody Nigel's alre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271561</th>\n",
       "      <td>0</td>\n",
       "      <td>1990040643</td>\n",
       "      <td>Mon Jun 01 03:40:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Stephany13329</td>\n",
       "      <td>long week ahead... actually long summer ahead....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Date     Query  \\\n",
       "418520          0  2061788339  Sat Jun 06 21:37:01 PDT 2009  NO_QUERY   \n",
       "447784          0  2068796410  Sun Jun 07 14:42:55 PDT 2009  NO_QUERY   \n",
       "326165          0  2008025357  Tue Jun 02 13:30:34 PDT 2009  NO_QUERY   \n",
       "481308          0  2179593331  Mon Jun 15 09:19:00 PDT 2009  NO_QUERY   \n",
       "271561          0  1990040643  Mon Jun 01 03:40:53 PDT 2009  NO_QUERY   \n",
       "\n",
       "               user_id                                              tweet  \n",
       "418520  Halfbeanchacha  @crazyycamille my dads being a jerk and won't ...  \n",
       "447784      VoniaPerna  @ mmm that does sound good, but i'm at work  H...  \n",
       "326165          audy86            Oooo god it's humid ! I hate humidity!   \n",
       "481308   sexyexecutive  Low toner error at 5:19pm  Bloody Nigel's alre...  \n",
       "271561   Stephany13329  long week ahead... actually long summer ahead....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 5000 tweets from the dataset\n",
    "training_data = raw_training_data.sample(n=5000)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the distribution of positive and negative tweets remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    2516\n",
       "4    2484\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.groupby('sentiment')['ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the interpretation of the results we are going to recode the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418520</th>\n",
       "      <td>negative</td>\n",
       "      <td>2061788339</td>\n",
       "      <td>Sat Jun 06 21:37:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Halfbeanchacha</td>\n",
       "      <td>@crazyycamille my dads being a jerk and won't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447784</th>\n",
       "      <td>negative</td>\n",
       "      <td>2068796410</td>\n",
       "      <td>Sun Jun 07 14:42:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>VoniaPerna</td>\n",
       "      <td>@ mmm that does sound good, but i'm at work  H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326165</th>\n",
       "      <td>negative</td>\n",
       "      <td>2008025357</td>\n",
       "      <td>Tue Jun 02 13:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>audy86</td>\n",
       "      <td>Oooo god it's humid ! I hate humidity!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481308</th>\n",
       "      <td>negative</td>\n",
       "      <td>2179593331</td>\n",
       "      <td>Mon Jun 15 09:19:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexyexecutive</td>\n",
       "      <td>Low toner error at 5:19pm  Bloody Nigel's alre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271561</th>\n",
       "      <td>negative</td>\n",
       "      <td>1990040643</td>\n",
       "      <td>Mon Jun 01 03:40:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Stephany13329</td>\n",
       "      <td>long week ahead... actually long summer ahead....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment          ID                          Date     Query  \\\n",
       "418520  negative  2061788339  Sat Jun 06 21:37:01 PDT 2009  NO_QUERY   \n",
       "447784  negative  2068796410  Sun Jun 07 14:42:55 PDT 2009  NO_QUERY   \n",
       "326165  negative  2008025357  Tue Jun 02 13:30:34 PDT 2009  NO_QUERY   \n",
       "481308  negative  2179593331  Mon Jun 15 09:19:00 PDT 2009  NO_QUERY   \n",
       "271561  negative  1990040643  Mon Jun 01 03:40:53 PDT 2009  NO_QUERY   \n",
       "\n",
       "               user_id                                              tweet  \n",
       "418520  Halfbeanchacha  @crazyycamille my dads being a jerk and won't ...  \n",
       "447784      VoniaPerna  @ mmm that does sound good, but i'm at work  H...  \n",
       "326165          audy86            Oooo god it's humid ! I hate humidity!   \n",
       "481308   sexyexecutive  Low toner error at 5:19pm  Bloody Nigel's alre...  \n",
       "271561   Stephany13329  long week ahead... actually long summer ahead....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recode_sentiment(series):\n",
    "    if series == 4:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "training_data['sentiment'] = training_data['sentiment'].apply(recode_sentiment)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Preprocessing\n",
    "\n",
    "At this step, we will preprocess the text in the tweets, tokenize and stem it. We will have to take care of specific markups (e.g., hashtags) related to Twitter, as well as of other aspects related to the sentiment analysis, like, for instance, emoticons.\n",
    "\n",
    "In the following, I give you an example of processing. I will use regular expressions to detect hashtags and change the detected hashtag by an indicator of the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "A hashtag is a word or an un-spaced phrase prefixed with the hash symbol (#). These are used to both naming subjects and phrases that are currently in trending topics. For example, #iPad, #news\n",
    "\n",
    "    Regular Expression: #(\\w+)\n",
    "\n",
    "    Replace Expression: HASH_hashtag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "\n",
    "def hash_repl(match):\n",
    "    \"\"\"\n",
    "    Detect hashtags and create a new feature: _HASH_+text of the hashtag\n",
    "    \"\"\"\n",
    "    return '__HASH_'+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this function, we will make use of the `re.sub` function. This function takes a regular expression (`hash_regex` in our case) and a replacing function (our `hash_repl` function) to change every appearance of the regular expression to the output of the replacing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy midsummer everyone! My little brother has a bd today and here are few relatives having a dinner.. not so sure is it very nice and __HASH_HASHTAG'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the created function\n",
    "re.sub( hash_regex, hash_repl, 'happy midsummer everyone! My little brother has a bd today and here are few relatives having a dinner.. not so sure is it very nice and #hashtag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: More pre-processing\n",
    "\n",
    "Following the previous example, create more regex and functions to detect some other twitter-related aspects (e.g., user names, URLs, emoticons, punctuations, repetitions, stemming, ...)\n",
    "\n",
    "You may find interesting ideas in this regard in the following links:\n",
    " - Christopher Potts sentiment tokenizer: http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    " - Brendan Oâ€™Connor twitter tokenizer: https://github.com/brendano/tweetmotif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate the application of these pre-processing steps, we will create a function to enclose all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the processing procedures\n",
    "def processAll(text):\n",
    "    \n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    \n",
    "    # All your pre-processing steps here\n",
    "    \n",
    "    if not isinstance(text,list):\n",
    "        text = text.split() # To avoid format errors\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column in our dataframe with the processed text by applying our `processAll` function to all the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418520</th>\n",
       "      <td>negative</td>\n",
       "      <td>2061788339</td>\n",
       "      <td>Sat Jun 06 21:37:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Halfbeanchacha</td>\n",
       "      <td>@crazyycamille my dads being a jerk and won't ...</td>\n",
       "      <td>[@crazyycamille, my, dads, being, a, jerk, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447784</th>\n",
       "      <td>negative</td>\n",
       "      <td>2068796410</td>\n",
       "      <td>Sun Jun 07 14:42:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>VoniaPerna</td>\n",
       "      <td>@ mmm that does sound good, but i'm at work  H...</td>\n",
       "      <td>[@, mmm, that, does, sound, good,, but, i'm, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326165</th>\n",
       "      <td>negative</td>\n",
       "      <td>2008025357</td>\n",
       "      <td>Tue Jun 02 13:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>audy86</td>\n",
       "      <td>Oooo god it's humid ! I hate humidity!</td>\n",
       "      <td>[Oooo, god, it's, humid, !, I, hate, humidity!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481308</th>\n",
       "      <td>negative</td>\n",
       "      <td>2179593331</td>\n",
       "      <td>Mon Jun 15 09:19:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexyexecutive</td>\n",
       "      <td>Low toner error at 5:19pm  Bloody Nigel's alre...</td>\n",
       "      <td>[Low, toner, error, at, 5:19pm, Bloody, Nigel'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271561</th>\n",
       "      <td>negative</td>\n",
       "      <td>1990040643</td>\n",
       "      <td>Mon Jun 01 03:40:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Stephany13329</td>\n",
       "      <td>long week ahead... actually long summer ahead....</td>\n",
       "      <td>[long, week, ahead..., actually, long, summer,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment          ID                          Date     Query  \\\n",
       "418520  negative  2061788339  Sat Jun 06 21:37:01 PDT 2009  NO_QUERY   \n",
       "447784  negative  2068796410  Sun Jun 07 14:42:55 PDT 2009  NO_QUERY   \n",
       "326165  negative  2008025357  Tue Jun 02 13:30:34 PDT 2009  NO_QUERY   \n",
       "481308  negative  2179593331  Mon Jun 15 09:19:00 PDT 2009  NO_QUERY   \n",
       "271561  negative  1990040643  Mon Jun 01 03:40:53 PDT 2009  NO_QUERY   \n",
       "\n",
       "               user_id                                              tweet  \\\n",
       "418520  Halfbeanchacha  @crazyycamille my dads being a jerk and won't ...   \n",
       "447784      VoniaPerna  @ mmm that does sound good, but i'm at work  H...   \n",
       "326165          audy86            Oooo god it's humid ! I hate humidity!    \n",
       "481308   sexyexecutive  Low toner error at 5:19pm  Bloody Nigel's alre...   \n",
       "271561   Stephany13329  long week ahead... actually long summer ahead....   \n",
       "\n",
       "                                          processed_tweet  \n",
       "418520  [@crazyycamille, my, dads, being, a, jerk, and...  \n",
       "447784  [@, mmm, that, does, sound, good,, but, i'm, a...  \n",
       "326165    [Oooo, god, it's, humid, !, I, hate, humidity!]  \n",
       "481308  [Low, toner, error, at, 5:19pm, Bloody, Nigel'...  \n",
       "271561  [long, week, ahead..., actually, long, summer,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['processed_tweet'] = training_data.tweet.apply(processAll)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation\n",
    "\n",
    "A wide variety of features can be used to build a classifier for tweets. The most widely used and basic feature set is word n-grams. However, there's a lot of domain specific information present in tweets that can also be used for classifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams\n",
    "\n",
    "Unigrams are the simplest features that can be used for text classification. A Tweet can be represented by a multiset of words present in it. We, however, have used the presence of unigrams in a tweet as a feature set. Presence of a word is more important than how many times it is repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example': 1, 'of': 1, 'tweet': 1, 'represented': 1, 'as': 1, 'unigrams': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"Example\", \"of\", \"tweet\", \"represented\", \"as\", \"unigrams\"]\n",
    "\n",
    "unigrams_fd = nltk.FreqDist()\n",
    "unigrams_fd.update(text)\n",
    "unigrams_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "N-gram refers to an n-long sequence of words. Probabilistic Language Models based on Unigrams, Bigrams and Trigrams can be successfully used to predict the next word given a current context of words. In the domain of sentiment analysis, the performance of N-grams is unclear.\n",
    "\n",
    "As the order of the n-grams increases, they tend to be more and more sparse. Let's then try bi-gram and tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example,of': 1, 'of,tweet': 1, 'tweet,represented': 1, 'represented,as': 1, 'as,unigrams': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams\n",
    "words_bi  = [ ','.join(map(str,bg)) for bg in nltk.bigrams(text) ]\n",
    "bi_grams_fd = nltk.FreqDist()\n",
    "bi_grams_fd.update( words_bi )\n",
    "bi_grams_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Example,of,tweet': 1, 'of,tweet,represented': 1, 'tweet,represented,as': 1, 'represented,as,unigrams': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigrams\n",
    "words_tri  = [ ','.join(map(str,tg)) for tg in nltk.trigrams(text) ]\n",
    "tri_grams_fd = nltk.FreqDist()\n",
    "tri_grams_fd.update( words_tri )\n",
    "tri_grams_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the bigrams and trigrams models for the processed text in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the n-grams procedures\n",
    "\n",
    "def get_word_features(words):\n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    words_bi  = [ 'has(%s)'% ','.join(map(str,bg)) for bg in nltk.bigrams(words) ]\n",
    "    words_tri = [ 'has(%s)'% ','.join(map(str,tg)) for tg in nltk.trigrams(words) ]\n",
    "    \n",
    "    for f in words_uni+words_bi+words_tri:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negations\n",
    "\n",
    "The need negation detection in sentiment analysis can be illustrated by the difference in the meaning of the phrases, \"This is good\" vs. \"This is not good\" However, the negations occurring in natural language are seldom so simple. Handling the negation consists of two tasks â€“ Detection of explicit negation cues and the scope of negation of these words.\n",
    "\n",
    "**Scope of Negation**\n",
    "\n",
    "Words immediately preceding and following the negation cues are the most negative and the words that come farther away do not lie in the scope of negation of such cues. We define left and right negativity of a word as the chances that meaning of that word is actually the opposite. Left negativity depends on the closest negation cue on the left and similarly for Right negativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "negtn_regex = re.compile( r\"\"\"(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "    )$\n",
    ")\n",
    "|\n",
    "n't\n",
    "\"\"\", re.X)\n",
    "\n",
    "def get_negation_features(words):\n",
    "    INF = 0.0\n",
    "    negtn = [ bool(negtn_regex.search(w)) for w in words ]\n",
    "\n",
    "    left = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in range(0,len(words)):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        left[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    right = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in reversed(range(0,len(words))):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        right[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    return dict( zip(\n",
    "                    ['neg_l('+w+')' for w in  words] + ['neg_r('+w+')' for w in  words],\n",
    "                    left + right ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_l(This)': 0.0,\n",
       " 'neg_l(text)': 0.0,\n",
       " 'neg_l(does)': 0.0,\n",
       " 'neg_l(not)': 1.0,\n",
       " 'neg_l(have)': 0.9,\n",
       " 'neg_l(a)': 0.8,\n",
       " 'neg_l(negation)': 0.7000000000000001,\n",
       " 'neg_r(This)': 0.7000000000000001,\n",
       " 'neg_r(text)': 0.8,\n",
       " 'neg_r(does)': 0.9,\n",
       " 'neg_r(not)': 1.0,\n",
       " 'neg_r(have)': 0.0,\n",
       " 'neg_r(a)': 0.0,\n",
       " 'neg_r(negation)': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "text = [\"This\",\"text\", \"does\", \"not\", \"have\", \"a\", \"negation\"]\n",
    "get_negation_features(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "With POS Tagging we can get the category of each word. Some of these categories are more interesting in order to infer the sentiment of given tweet. For example, adjectives are expected to carry most sentiment information than adverbs. In a similar way, some particular names can carry a positive or negative implication for particular domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_features(words):\n",
    "    tags = {}\n",
    "    tagged_words = [ 'has(%s)'% w+'_'+tag for w,tag in nltk.pos_tag(words) if len(words) > 0]\n",
    "    \n",
    "    for tw in tagged_words:\n",
    "        tags[tw] = 1\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous step, let's create an function to apply all these creation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for the extraction of features\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    \n",
    "    words = processAll(text)\n",
    "    \n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "    \n",
    "    pos_features = get_pos_features(words)\n",
    "    features.update( pos_features )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418520</th>\n",
       "      <td>@crazyycamille my dads being a jerk and won't ...</td>\n",
       "      <td>{'has(@crazyycamille)': 1, 'has(my)': 1, 'has(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447784</th>\n",
       "      <td>@ mmm that does sound good, but i'm at work  H...</td>\n",
       "      <td>{'has(@)': 1, 'has(mmm)': 1, 'has(that)': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326165</th>\n",
       "      <td>Oooo god it's humid ! I hate humidity!</td>\n",
       "      <td>{'has(Oooo)': 1, 'has(god)': 1, 'has(it's)': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481308</th>\n",
       "      <td>Low toner error at 5:19pm  Bloody Nigel's alre...</td>\n",
       "      <td>{'has(Low)': 1, 'has(toner)': 1, 'has(error)':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271561</th>\n",
       "      <td>long week ahead... actually long summer ahead....</td>\n",
       "      <td>{'has(long)': 1, 'has(week)': 1, 'has(ahead......</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  \\\n",
       "418520  @crazyycamille my dads being a jerk and won't ...   \n",
       "447784  @ mmm that does sound good, but i'm at work  H...   \n",
       "326165            Oooo god it's humid ! I hate humidity!    \n",
       "481308  Low toner error at 5:19pm  Bloody Nigel's alre...   \n",
       "271561  long week ahead... actually long summer ahead....   \n",
       "\n",
       "                                 processed_tweet_features  \n",
       "418520  {'has(@crazyycamille)': 1, 'has(my)': 1, 'has(...  \n",
       "447784  {'has(@)': 1, 'has(mmm)': 1, 'has(that)': 1, '...  \n",
       "326165  {'has(Oooo)': 1, 'has(god)': 1, 'has(it's)': 1...  \n",
       "481308  {'has(Low)': 1, 'has(toner)': 1, 'has(error)':...  \n",
       "271561  {'has(long)': 1, 'has(week)': 1, 'has(ahead......  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['processed_tweet_features'] = training_data.tweet.apply(extract_features)\n",
    "training_data[['tweet','processed_tweet_features']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Let's now use the processed tweet features to create a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-test Splitting\n",
    "\n",
    "To evaluate our approaches, we are going to split our data into train and validation. We will use the train to create the models and the validation to validate their performance. Once we have selected the best model (according to the accuracy on the validation set) we can use this model to predict our test set.\n",
    "\n",
    "In this way, test set will remain as unseen data for all the process: we are not going to make any decision based on the test error. Therefore, we can assume that the results on the test set will be the same that we will obtain when new unseen data appears in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 4000\n",
    "train_tweets = [(tweet, sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[:training_size]]\n",
    "validation_tweets  = [(tweet, sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[training_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for the classifier\n",
    "\n",
    "We have previously defined a feature extraction process, which we have wrapped into the `extract_features` function.\n",
    "\n",
    "By making use of the `nltk.classify.apply_features` function provided by NLTK, we will process the tweets and create the features that will be used for the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the data processing and cleaning extraction methodologies\n",
    "v_train = nltk.classify.apply_features(extract_features,train_tweets)\n",
    "v_validation  = nltk.classify.apply_features(extract_features,validation_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the resultant object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the tweet =  @crazyycamille my dads being a jerk and won't buy me snowballs. That made me miss you and your snowballs. \n",
      " \n",
      "The following features has been created:\n",
      " \n",
      "{'has(@crazyycamille)': 1, 'has(my)': 1, 'has(dads)': 1, 'has(being)': 1, 'has(a)': 1, 'has(jerk)': 1, 'has(and)': 1, \"has(won't)\": 1, 'has(buy)': 1, 'has(me)': 1, 'has(snowballs.)': 1, 'has(That)': 1, 'has(made)': 1, 'has(miss)': 1, 'has(you)': 1, 'has(your)': 1, 'has(@crazyycamille,my)': 1, 'has(my,dads)': 1, 'has(dads,being)': 1, 'has(being,a)': 1, 'has(a,jerk)': 1, 'has(jerk,and)': 1, \"has(and,won't)\": 1, \"has(won't,buy)\": 1, 'has(buy,me)': 1, 'has(me,snowballs.)': 1, 'has(snowballs.,That)': 1, 'has(That,made)': 1, 'has(made,me)': 1, 'has(me,miss)': 1, 'has(miss,you)': 1, 'has(you,and)': 1, 'has(and,your)': 1, 'has(your,snowballs.)': 1, 'has(@crazyycamille,my,dads)': 1, 'has(my,dads,being)': 1, 'has(dads,being,a)': 1, 'has(being,a,jerk)': 1, 'has(a,jerk,and)': 1, \"has(jerk,and,won't)\": 1, \"has(and,won't,buy)\": 1, \"has(won't,buy,me)\": 1, 'has(buy,me,snowballs.)': 1, 'has(me,snowballs.,That)': 1, 'has(snowballs.,That,made)': 1, 'has(That,made,me)': 1, 'has(made,me,miss)': 1, 'has(me,miss,you)': 1, 'has(miss,you,and)': 1, 'has(you,and,your)': 1, 'has(and,your,snowballs.)': 1, 'neg_l(@crazyycamille)': 0.0, 'neg_l(my)': 0.0, 'neg_l(dads)': 0.0, 'neg_l(being)': 0.0, 'neg_l(a)': 0.0, 'neg_l(jerk)': 0.0, 'neg_l(and)': 0.10000000000000014, \"neg_l(won't)\": 1.0, 'neg_l(buy)': 0.9, 'neg_l(me)': 0.40000000000000013, 'neg_l(snowballs.)': 0.0, 'neg_l(That)': 0.6000000000000001, 'neg_l(made)': 0.5000000000000001, 'neg_l(miss)': 0.30000000000000016, 'neg_l(you)': 0.20000000000000015, 'neg_l(your)': 1.3877787807814457e-16, 'neg_r(@crazyycamille)': 0.30000000000000016, 'neg_r(my)': 0.40000000000000013, 'neg_r(dads)': 0.5000000000000001, 'neg_r(being)': 0.6000000000000001, 'neg_r(a)': 0.7000000000000001, 'neg_r(jerk)': 0.8, 'neg_r(and)': 0.0, \"neg_r(won't)\": 1.0, 'neg_r(buy)': 0.0, 'neg_r(me)': 0.0, 'neg_r(snowballs.)': 0.0, 'neg_r(That)': 0.0, 'neg_r(made)': 0.0, 'neg_r(miss)': 0.0, 'neg_r(you)': 0.0, 'neg_r(your)': 0.0, 'has(@crazyycamille)_VB': 1, 'has(my)_PRP$': 1, 'has(dads)_NNS': 1, 'has(being)_VBG': 1, 'has(a)_DT': 1, 'has(jerk)_NN': 1, 'has(and)_CC': 1, \"has(won't)_NN\": 1, 'has(buy)_VB': 1, 'has(me)_PRP': 1, 'has(snowballs.)_VB': 1, 'has(That)_DT': 1, 'has(made)_VBD': 1, 'has(miss)_VB': 1, 'has(you)_PRP': 1, 'has(your)_PRP$': 1, 'has(snowballs.)_NN': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"For the tweet = \", training_data.tweet.values[0])\n",
    "print(\" \")\n",
    "print(\"The following features has been created:\")\n",
    "print(\" \")\n",
    "print(v_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "We will start with a simple NaÃ¯ve Bayes Classifier. For a given tweet, if we need to find the label for it, we find the probabilities of all the labels, given that feature and then select the label with maximum probability.\n",
    "\n",
    "NLTK has its own implementation of Naive Bayes `nltk.classify.NaiveBayesClassifier`. If you prefer, you can use the Naive Bayes implementation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = nltk.classify.NaiveBayesClassifier\n",
    "nb_class = nb_classifier.train(v_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Let's evaluate the accuracy of our model in our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model =  0.669\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model = \", nltk.classify.accuracy(nb_class, v_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71.4 % of accuracy seems pretty good for the task.\n",
    "\n",
    "We can have a more detailed idea of the performance by taking a look to the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative |<365>112 |\n",
      "positive | 219<304>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_validation]\n",
    "test_predict = [nb_class.classify(t) for (t,s) in v_validation]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Representative Features\n",
    "\n",
    "The NLTK classifier object allows us to see the most representative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             has(sad)_JJ = 1              negati : positi =     24.0 : 1.0\n",
      "             neg_r(hate) = 0.0            negati : positi =     14.4 : 1.0\n",
      "                has(sad) = 1              negati : positi =     14.4 : 1.0\n",
      "             neg_l(hate) = 0.0            negati : positi =     14.0 : 1.0\n",
      "              neg_l(sad) = 0.0            negati : positi =     13.1 : 1.0\n",
      "              neg_r(sad) = 0.0            negati : positi =     12.5 : 1.0\n",
      "             has(I,hate) = 1              negati : positi =     12.5 : 1.0\n",
      "             has(wish,I) = 1              negati : positi =     11.9 : 1.0\n",
      "             neg_r(away) = 0.0            negati : positi =     11.9 : 1.0\n",
      "            has(wish)_JJ = 1              negati : positi =     11.9 : 1.0\n",
      "               has(away) = 1              negati : positi =     11.9 : 1.0\n",
      "             neg_l(away) = 0.0            negati : positi =     11.2 : 1.0\n",
      "               has(lost) = 1              negati : positi =     11.2 : 1.0\n",
      "             neg_l(lost) = 0.0            negati : positi =     11.2 : 1.0\n",
      "          has(feel,like) = 1              negati : positi =     11.2 : 1.0\n",
      "               has(hate) = 1              negati : positi =     11.1 : 1.0\n",
      "              neg_r(and) = 0.7000000000000001 negati : positi =     10.6 : 1.0\n",
      "             neg_l(want) = 0.9            negati : positi =     10.6 : 1.0\n",
      "            has(sick)_JJ = 1              negati : positi =     10.6 : 1.0\n",
      "            has(away)_RB = 1              negati : positi =     10.6 : 1.0\n",
      "         neg_r(birthday) = 0.0            positi : negati =     10.1 : 1.0\n",
      "             neg_r(lost) = 0.0            negati : positi =      9.9 : 1.0\n",
      "             has(sad)_NN = 1              negati : positi =      9.3 : 1.0\n",
      "           has(hate)_VBP = 1              negati : positi =      8.9 : 1.0\n",
      "              neg_l(New) = 0.0            positi : negati =      8.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_class.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "We have applied a thorough process to create features for our tweets. However, is it justified? Have we actually created a better representation of our data? To know that, we are going to create a baseline model that uses only the text in the tweets (with no features added).\n",
    "\n",
    "To that end we define a new extraction function that only extract the terms from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train_tweets = [(tweet.split(\" \"), sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[:training_size]]\n",
    "baseline_validation_tweets  = [(tweet.split(\" \"), sentiment) for tweet, sentiment in training_data[['tweet', 'sentiment']].values[training_size:]]\n",
    "\n",
    "# Wrapper function for the extraction of features\n",
    "def extract_baseline_features(words):\n",
    "    \n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    \n",
    "    for f in words_uni:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag\n",
    "\n",
    "v_baseline_train = nltk.classify.apply_features(extract_baseline_features, baseline_train_tweets)\n",
    "v_baseline_validation = nltk.classify.apply_features(extract_baseline_features, baseline_validation_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a new naive based classifier over this baseline representation and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_nb_classifier = nltk.classify.NaiveBayesClassifier\n",
    "baseline_nb_class = nb_classifier.train(v_baseline_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the baseline model =  0.649\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the baseline model = \", nltk.classify.accuracy(baseline_nb_class, v_baseline_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative |<368>109 |\n",
      "positive | 243<280>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix over validation set\n",
    "test_truth   = [s for (t,s) in v_baseline_validation]\n",
    "test_predict = [nb_class.classify(t) for (t,s) in v_baseline_validation]\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print()\n",
    "print(nltk.ConfusionMatrix( test_truth, test_predict ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, performance is significantly lower than that of the model using all the features we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                has(sad) = 1              negati : positi =     14.4 : 1.0\n",
      "               has(away) = 1              negati : positi =     11.9 : 1.0\n",
      "               has(lost) = 1              negati : positi =     11.2 : 1.0\n",
      "               has(hate) = 1              negati : positi =     11.1 : 1.0\n",
      "                has(New) = 1              positi : negati =      8.7 : 1.0\n",
      "          has(followers) = 1              positi : negati =      8.7 : 1.0\n",
      "              has(sucks) = 1              negati : positi =      8.0 : 1.0\n",
      "             has(You're) = 1              positi : negati =      8.0 : 1.0\n",
      "            has(finally) = 1              positi : negati =      8.0 : 1.0\n",
      "               has(able) = 1              negati : positi =      7.4 : 1.0\n",
      "               has(wish) = 1              negati : positi =      7.3 : 1.0\n",
      "               has(wont) = 1              negati : positi =      6.7 : 1.0\n",
      "               has(open) = 1              negati : positi =      6.7 : 1.0\n",
      "            has(jealous) = 1              negati : positi =      6.7 : 1.0\n",
      "             has(thanks) = 1              positi : negati =      6.1 : 1.0\n",
      "             has(asleep) = 1              negati : positi =      6.1 : 1.0\n",
      "            has(wishing) = 1              negati : positi =      6.1 : 1.0\n",
      "             has(iPhone) = 1              negati : positi =      6.1 : 1.0\n",
      "              has(wrong) = 1              negati : positi =      6.1 : 1.0\n",
      "               has(shit) = 1              negati : positi =      6.1 : 1.0\n",
      "             has(stupid) = 1              negati : positi =      6.1 : 1.0\n",
      "           has(birthday) = 1              positi : negati =      6.0 : 1.0\n",
      "          has(chocolate) = 1              positi : negati =      5.9 : 1.0\n",
      "          has(listening) = 1              positi : negati =      5.6 : 1.0\n",
      "            has(weather) = 1              negati : positi =      5.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Most Representative Features\n",
    "baseline_nb_class.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: MaxEnt Classifier\n",
    "\n",
    "Let's try a more sophisticated classifier to see if we can boost the classification performance. In particular we will apply a Maximum Entropy Classifier. This classifier works by finding a probability distribution that maximizes the likelihood of testable data.\n",
    "\n",
    "To create a MaxEnt model, make use of the `nltk.classify.MaxentClassifier` function and follow the Naive Bayes example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the theoretical session we presented some sentiment resources that could be used to enrich our dataset with external information.\n",
    "\n",
    "In particular, SentiWordNet provides a sentiment annotation for the WordNet synsets. We can add this sentiment annotation as new features to our dataset. \n",
    "\n",
    "In the following, we define a fuction that based on the words in the tweets and their POS tagging, find the sentiment annotation for the word_POS_TAG in SentiWordNet. We then add these values as new features in our dataset and use them to train a new MaxEnt Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\madcastea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\madcastea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\sentiwordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download the Wordnet Corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download the Senti Wordnet Corpus\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    " \n",
    "\n",
    "def swn_polarity(text):\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "  \n",
    "    tagged_sentence = pos_tag(word_tokenize(text))\n",
    "    sentiment = {}\n",
    "    for word, tag in tagged_sentence:\n",
    "        \n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "        \n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            sentiment[\"sent(\"+word+\")\"] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Take the first sense, the most common\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "        sentiment[\"sent(\"+word+\")\"] = swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent(This)': 0.0,\n",
       " 'sent(is)': 0.0,\n",
       " 'sent(a)': 0.0,\n",
       " 'sent(text)': 0.0,\n",
       " 'sent(with)': 0.0,\n",
       " 'sent(good)': 0.75,\n",
       " 'sent(and)': 0.0,\n",
       " 'sent(very)': 0.0,\n",
       " 'sent(words)': 0.0,\n",
       " 'sent(bad)': -0.625,\n",
       " 'sent(stupid)': -0.75}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a text with good and very good words and bad and stupid words\"\n",
    "swn_polarity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This annotation provides a sentiment score (based on the SentiWordNet sentiment score) for each term in the tweets (-1 negative, 1 positive, 0 neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for the extraction of features + sentiment features\n",
    "def extract_features_with_sentiment(text):\n",
    "    features = {}\n",
    "    \n",
    "    words = processAll(text)\n",
    "    \n",
    "    sentiment_features = swn_polarity(text)\n",
    "    features.update(sentiment_features)\n",
    "    \n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Enhanced classifier\n",
    "\n",
    "We are going to test if the sentiment lexicon improves our MaxEnt classifier. To that end you have to make use of the `extract_features_with_sentiment` function to create the features (by using the `nltk.classify.apply_features` function) to feed the classifier. **(take a look to the Naive Bayes example)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
